apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: iris-training-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"train-model": [{"key": "artifacts/$PIPELINERUN/train-model/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/outputs/mlpipeline_ui_metadata/data"},
      {"key": "artifacts/$PIPELINERUN/train-model/mlpipeline-metrics.tgz", "name":
      "mlpipeline-metrics", "path": "/tmp/outputs/mlpipeline_metrics/data"}]}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"train-model": [["mlpipeline-ui-metadata", "/tmp/outputs/mlpipeline_ui_metadata/data"],
      ["mlpipeline-metrics", "/tmp/outputs/mlpipeline_metrics/data"]], "upload-iris-data":
      []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"name": "iris-training-pipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: upload-iris-data
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_iris_data():

                from sklearn import datasets
                import pandas as pd
                import numpy as np
                from minio import Minio
                from sklearn.model_selection import train_test_split
                from sklearn.ensemble import RandomForestClassifier

            ##### Helper functions
                def load_iris_data():
                    iris = datasets.load_iris()
                    data=pd.DataFrame({
                        'sepal length':iris.data[:,0],
                        'sepal width':iris.data[:,1],
                        'petal length':iris.data[:,2],
                        'petal width':iris.data[:,3],
                        'species':iris.target
                    })
                    data.head()
                    return data

                def load_minio():
                    minio_client = Minio(
                        "minio-service.mlops-demo-datascience.svc.cluster.local:9000",
                        access_key="minio",
                        secret_key="minio123",
                        secure=False
                    )
                    minio_bucket = "mlpipeline"

                    return minio_client, minio_bucket

            #############################################################

                # Get Data from Iris Data Set and push to Minio Storage
                iris_data = load_iris_data()
                minio_client, minio_bucket = load_minio()

                X=iris_data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features
                y=iris_data['species']  # Labels

                # Split dataset into training set and test set
                x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.01) # 70% training and 30% test

                # save to numpy file, store in Minio
                np.save("/tmp/x_train.npy",x_train)
                minio_client.fput_object(minio_bucket,"x_train","/tmp/x_train.npy")

                np.save("/tmp/y_train.npy",y_train)
                minio_client.fput_object(minio_bucket,"y_train","/tmp/y_train.npy")

                np.save("/tmp/x_test.npy",x_test)
                minio_client.fput_object(minio_bucket,"x_test","/tmp/x_test.npy")

                np.save("/tmp/y_test.npy",y_test)
                minio_client.fput_object(minio_bucket,"y_test","/tmp/y_test.npy")

                print(f"x_train shape: {x_train.shape}")
                print(f"y_train shape: {y_train.shape}")

                print(f"x_test shape: {x_test.shape}")
                print(f"y_test shape: {y_test.shape}")

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload iris data', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_iris_data(**_parsed_args)
          image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-scipy:v1.5.0
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload iris data",
              "outputs": [], "version": "Upload iris data@sha256=329194ee461c81d118c9f2b4c39f84131c8326e3e6d01e36e26b6764b7f470ea"}'
    - name: train-model
      taskSpec:
        steps:
        - name: main
          args:
          - '----output-paths'
          - /tmp/outputs/mlpipeline_ui_metadata/data
          - /tmp/outputs/mlpipeline_metrics/data
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def train_model():

                import numpy as np
                from minio import Minio
                from sklearn.ensemble import RandomForestClassifier
                import pickle
                from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
                import json

            ########## HELPER FUNCTIONS #############################
                def load_minio():
                    minio_client = Minio(
                        "minio-service.mlops-demo-datascience.svc.cluster.local:9000",
                        access_key="minio",
                        secret_key="minio123",
                        secure=False
                    )
                    minio_bucket = "mlpipeline"

                    return minio_client, minio_bucket

                def load_model_into_s3(filename, model_pkle):
                    #Ensure container env has boto3
                    #import boto3
                    print("In Progress")

            #########################################

                # Create Model and Train
                minio_client, minio_bucket = load_minio()
                minio_client.fget_object(minio_bucket,"x_train","/tmp/x_train.npy")
                x_train = np.load("/tmp/x_train.npy")

                minio_client.fget_object(minio_bucket,"y_train","/tmp/y_train.npy")
                y_train = np.load("/tmp/y_train.npy")

                minio_client.fget_object(minio_bucket,"x_test","/tmp/x_test.npy")
                x_test = np.load("/tmp/x_test.npy")

                minio_client.fget_object(minio_bucket,"y_test","/tmp/y_test.npy")
                y_test = np.load("/tmp/y_test.npy")

                #Create a Gaussian Classifier
                model=RandomForestClassifier(n_estimators=100)

                #Train the model using the training sets y_pred=clf.predict(X_test)
                model.fit(x_train,y_train)
                y_pred=model.predict(x_test)

                print(x_train)
                model.predict([[5,3,1.6,0.2]])

                filename = '/tmp/iris-model.pkl'
                pickle.dump(model, open(filename, 'wb'))

                # Upload model to minio
                minio_client.fput_object(minio_bucket, "iris-model", filename)

                # Upload Model into S3
                load_model_into_s3("iris-model", filename)
                # Output metrics

                confusion_matrix_metric = confusion_matrix(y_test, y_pred)
                accuracy_score_metric = accuracy_score(y_test, y_pred)
                classification_report_metric = classification_report(y_test,y_pred)

                metrics = {
                  'metrics': [{
                      'name': 'model_accuracy',
                      'numberValue':  float(accuracy_score_metric),
                      'format' : "PERCENTAGE"
                    }]}
                metadata = {
                  'metadata': [{
                      'placeholder_key': 'placeholder_value'
                    }]}

                from collections import namedtuple
                output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])
                return output(json.dumps(metadata),json.dumps(metrics))

            import argparse
            _parser = argparse.ArgumentParser(prog='Train model', description='')
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = train_model(**_parsed_args)

            _output_serializers = [
                str,
                str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-scipy:v1.5.0
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp/outputs/mlpipeline_ui_metadata
          - name: mlpipeline-metrics
            mountPath: /tmp/outputs/mlpipeline_metrics
        volumes:
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        - name: mlpipeline-metrics
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Train model",
              "outputs": [{"name": "mlpipeline_ui_metadata", "type": "UI_metadata"},
              {"name": "mlpipeline_metrics", "type": "Metrics"}], "version": "Train
              model@sha256=02c7f4eb59d22cb4735fcfff6bbe2082b42d8c45589ac5e966940d29c81a3357"}'
      runAfter:
      - upload-iris-data
